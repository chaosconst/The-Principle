import requests
from bs4 import BeautifulSoup
import time
import os
import re

# èµ·å§‹ URL (ä¸»é¡µ)
start_url = "https://shcontrolstyle.blogspot.com/?m=1"
output_file = os.path.expanduser("~/pob_server/uploads/shcontrol_full_archive.txt")

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

visited_links = set()
all_posts = []

def fetch_post_content(title, url):
    print(f"    â¬‡ï¸ Fetching: {title}")
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å°è¯•æå–æ­£æ–‡
        content_div = soup.find(class_='post-body') or soup.find(class_='entry-content')
        
        if content_div:
            # å¤„ç†æ¢è¡Œ
            for br in content_div.find_all("br"):
                br.replace_with("\n")
            text = content_div.get_text(separator='\n', strip=True)
        else:
            # Fallback: è·å–æ‰€æœ‰æ–‡æœ¬ï¼Œä½†è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
            lines = [line.strip() for line in soup.get_text(separator='\n').split('\n') if len(line.strip()) > 50]
            text = "\n".join(lines)
            
        post_data = f"\n{'='*60}\nTITLE: {title}\nURL: {url}\n{'='*60}\n\n{text}\n"
        all_posts.append(post_data)
        
    except Exception as e:
        print(f"    âŒ Failed: {e}")

def process_page(url):
    print(f"ğŸ“„ Scanning Page: {url}...")
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. æš´åŠ›æå–æ–‡ç« é“¾æ¥
        page_links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            # Blogspot æ–‡ç« é“¾æ¥ç‰¹å¾: /yyyy/mm/xxx.html
            if re.search(r'/\d{4}/\d{2}/.*\.html', href) and '#comment' not in href:
                # æ’é™¤ archive ä¾§è¾¹æ é“¾æ¥ (é€šå¸¸åªæœ‰æœˆä»½æ²¡æœ‰æ ‡é¢˜ï¼Œæˆ–è€…æ ‡é¢˜æ˜¯æ—¥æœŸ)
                title = a.get_text(strip=True)
                if title and len(title) > 4 and href not in visited_links:
                    visited_links.add(href)
                    page_links.append((title, href))
        
        # å»é‡å¹¶æŠ“å–
        # æœ‰æ—¶å€™åŒä¸€ä¸ªé“¾æ¥ä¼šå‡ºç°å¤šæ¬¡ï¼ˆæ ‡é¢˜ã€é˜…è¯»æ›´å¤šç­‰ï¼‰ï¼Œç”¨ set è¿‡æ»¤
        unique_page_links = list({v: k for k, v in page_links}.items()) # href: title
        
        print(f"  Found {len(unique_page_links)} new posts.")
        
        for href, title in unique_page_links:
            fetch_post_content(title, href)
            time.sleep(0.5)

        # 2. å¯»æ‰¾ä¸‹ä¸€é¡µ
        next_link = soup.select_one('a.blog-pager-older-link')
        if next_link:
            next_url = next_link['href']
            print(f"  ğŸ‘‰ Next Page: {next_url}")
            time.sleep(1)
            process_page(next_url)
        else:
            print("âœ… Reached end of blog.")

    except Exception as e:
        print(f"âŒ Error processing page: {e}")

# å¼€å§‹
process_page(start_url)

# ä¿å­˜
if all_posts:
    # æŒ‰æ—¶é—´æ­£åºæ’åˆ— (å‡è®¾æŠ“å–æ˜¯ä»æ–°åˆ°æ—§ï¼Œæ‰€ä»¥åè½¬)
    all_posts.reverse()
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"SHCONTROL BLOG ARCHIVE\nGenerated: {time.strftime('%Y-%m-%d')}\nTotal Posts: {len(all_posts)}\n\n")
        for post in all_posts:
            f.write(post)
    print(f"\nğŸ‰ Archive saved to: {output_file} ({len(all_posts)} posts)")
else:
    print("\nâš ï¸ No posts found.")
