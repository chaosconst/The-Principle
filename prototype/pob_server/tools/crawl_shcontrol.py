import requests
from bs4 import BeautifulSoup
import time
import os
import re

start_url = "https://shcontrolstyle.blogspot.com/search?updated-max=2015-01-01T00:00:00-07:00&max-results=50&by-date=false&m=1"
output_file = os.path.expanduser("~/pob_server/uploads/shcontrol_full_archive.txt")

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

visited_links = set()
all_posts = []

def fetch_posts_from_page(url):
    print(f"ğŸ“„ Crawling page: {url}...")
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ–‡ç« é“¾æ¥
        # Blogspot ç§»åŠ¨ç‰ˆé€šå¸¸åœ¨ h3.mobile-index-title a
        links = soup.select('h3.mobile-index-title a')
        if not links:
            links = soup.select('h3.post-title a')
            
        for link in links:
            href = link['href']
            title = link.get_text(strip=True)
            
            if href not in visited_links:
                visited_links.add(href)
                print(f"  Found post: {title}")
                # è·å–æ–‡ç« å†…å®¹
                fetch_post_content(title, href)

        # å¯»æ‰¾ä¸‹ä¸€é¡µ
        next_link = soup.select_one('a.blog-pager-older-link')
        if next_link:
            time.sleep(1) # ç¤¼è²Œçˆ¬å–
            fetch_posts_from_page(next_link['href'])
        else:
            print("âœ… Reached end of blog.")

    except Exception as e:
        print(f"âŒ Error crawling page: {e}")

def fetch_post_content(title, url):
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_div = soup.find(class_='post-body')
        if content_div:
            # å¤„ç†æ¢è¡Œï¼ŒæŠŠ <br> æ¢æˆ \n
            for br in content_div.find_all("br"):
                br.replace_with("\n")
            text = content_div.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
            
        post_data = f"\n{'='*40}\nTITLE: {title}\nURL: {url}\n{'='*40}\n\n{text}\n"
        all_posts.append(post_data)
        
    except Exception as e:
        print(f"  âŒ Failed to fetch content: {e}")

# å¼€å§‹çˆ¬å–
# ç”±äº Blogspot çš„ç»“æ„ï¼Œç›´æ¥ä»ä¸»é¡µæˆ–è€…ä¸€ä¸ªè¾ƒæ–°çš„æ—¥æœŸå¼€å§‹
# è¿™é‡Œå°è¯•ä»ä¸»é¡µå¼€å§‹ï¼Œæˆ–è€…ç›´æ¥ç”¨ä¹‹å‰çš„ search url
fetch_posts_from_page("https://shcontrolstyle.blogspot.com/?m=1")

# å†™å…¥æ–‡ä»¶
if all_posts:
    # å€’åºæ’åˆ—ï¼ˆæŒ‰æ—¶é—´æ­£åºï¼Œå¦‚æœæ˜¯ä»æ–°åˆ°æ—§çˆ¬çš„è¯ï¼‰
    # Blogspot é»˜è®¤ä»æ–°åˆ°æ—§ï¼Œæˆ‘ä»¬ç¿»è½¬ä¸€ä¸‹å˜æˆä»æ—§åˆ°æ–°é˜…è¯»
    all_posts.reverse()
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"SHCONTROL BLOG ARCHIVE\nGenerated: {time.strftime('%Y-%m-%d')}\nTotal Posts: {len(all_posts)}\n\n")
        for post in all_posts:
            f.write(post)
    print(f"\nğŸ‰ Archive saved to: {output_file}")
else:
    print("\nâš ï¸ No posts found.")

